{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 18:43:07.419988: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-23 18:43:07.420008: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import Xception, DenseNet201, MobileNetV2, DenseNet169\n",
    "from tensorflow.keras.layers import Softmax, ReLU, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.layers import Conv2D, GlobalAveragePooling1D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import convert_to_tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from os import path\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from tensorflow import one_hot\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (224, 224, 3)\n",
    "train_depth = 10\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "steps_per_epoch=20\n",
    "save_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pictures(data_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    species = sorted(glob(path.join(data_path, \"*\")))\n",
    "    for label_id, sname in enumerate(species):\n",
    "        image_names = sorted(glob(path.join(sname, \"*\")))\n",
    "        for iname in image_names:\n",
    "            data.append(np.array(Image.open(iname).convert(\"RGB\")))\n",
    "            labels.append(label_id)\n",
    "    return np.asarray(data, dtype='float32'), np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 18:43:10.369401: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-05-23 18:43:10.369435: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vlad-desktop): /proc/driver/nvidia/version does not exist\n",
      "2022-05-23 18:43:10.369725: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "base_model = Xception(input_tensor=Input(shape=image_shape), include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "cnt = train_depth\n",
    "while cnt > 0:\n",
    "    base_model.layers[-cnt].trainable = True\n",
    "    cnt -= 1\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(400))\n",
    "model.add(BatchNormalization())\n",
    "model.add(ReLU())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_pictures(\"data/train\")\n",
    "X_test, y_test = get_pictures(\"data/test\")\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# X_train = tf.convert_to_tensor(X_train)\n",
    "# X_test = tf.convert_to_tensor(X_test)\n",
    "n_classes = np.unique(y_train).shape[0]\n",
    "y_train = one_hot(y_train, n_classes)\n",
    "y_test = one_hot(y_test, n_classes)\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# X_train = (X_train - X_train.mean(axis=(1, 2, 3))) / X_train.std(axis=(1, 2, 3))\n",
    "# X_test = (X_test - X_test.mean(axis=(1, 2, 3))) / X_test.std(axis=(1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1536, 12), dtype=float32, numpy=\n",
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([300, 12])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 224, 224, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = ImageDataGenerator(zoom_range=0.15, horizontal_flip=True, rotation_range=15, \n",
    "                              height_shift_range=0.3, width_shift_range=0.2)\n",
    "train_generator = data_gen.flow(X_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "test_generator = data_gen.flow(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "48/48 [==============================] - 113s 2s/step - loss: 1.5223 - accuracy: 0.5469 - val_loss: 1.2256 - val_accuracy: 0.8367\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 110s 2s/step - loss: 0.5103 - accuracy: 0.8743 - val_loss: 0.8286 - val_accuracy: 0.8633\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 118s 2s/step - loss: 0.3306 - accuracy: 0.9219 - val_loss: 0.4802 - val_accuracy: 0.9500\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 128s 3s/step - loss: 0.2773 - accuracy: 0.9395 - val_loss: 0.3401 - val_accuracy: 0.9667\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 139s 3s/step - loss: 0.2036 - accuracy: 0.9577 - val_loss: 0.2475 - val_accuracy: 0.9500\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 128s 3s/step - loss: 0.1902 - accuracy: 0.9570 - val_loss: 0.1537 - val_accuracy: 0.9633\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 124s 3s/step - loss: 0.1757 - accuracy: 0.9564 - val_loss: 0.1571 - val_accuracy: 0.9533\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 135s 3s/step - loss: 0.1408 - accuracy: 0.9655 - val_loss: 0.0921 - val_accuracy: 0.9833\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 130s 3s/step - loss: 0.1301 - accuracy: 0.9688 - val_loss: 0.1088 - val_accuracy: 0.9700\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 125s 3s/step - loss: 0.1321 - accuracy: 0.9661 - val_loss: 0.0985 - val_accuracy: 0.9767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 19:05:04.852559: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./assets\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, epochs=epochs, validation_data=test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model10epoches.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data_path, n_classes):\n",
    "    X_test, y_test = get_pictures(test_data_path)\n",
    "    X_test = X_test / 255.0\n",
    "    y_test = one_hot(y_test, n_classes)\n",
    "    model.evaluate(X_test, y_test)\n",
    "\n",
    "test_model(model=model, test_data_path=\"data/test\", n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 20:25:52.788030: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1104875520 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "def predict_stolen_labels(model, test_data_path, n_classes):\n",
    "    X_test, y_test = get_pictures(test_data_path)\n",
    "    X_test = X_test / 255.0\n",
    "    y_test = one_hot(y_test, n_classes)\n",
    "    return model.predict(X_test)\n",
    "predict_stolen_labels = predict_stolen_labels(model=model, test_data_path=\"data/stolen_train\", n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_stolen = np.argmax(predict_stolen_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_stolen = one_hot(y_test_stolen, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model2 = Xception(input_tensor=Input(shape=image_shape), include_top=False)\n",
    "base_model2.trainable = False\n",
    "\n",
    "cnt = train_depth\n",
    "while cnt > 0:\n",
    "    base_model2.layers[-cnt].trainable = True\n",
    "    cnt -= 1\n",
    "model2 = Sequential()\n",
    "model2.add(base_model2)\n",
    "model2.add(GlobalAveragePooling2D())\n",
    "model2.add(Dense(400))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(ReLU())\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(12, activation='softmax'))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_pictures(\"data/stolen_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (1964, 224, 224, 3), y.shape = (1835, 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m data_gen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(zoom_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, \n\u001b[1;32m      2\u001b[0m                               height_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m \u001b[43mdata_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_stolen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m data_gen\u001b[38;5;241m.\u001b[39mflow(X_test, y_test, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.9/site-packages/keras/preprocessing/image.py:884\u001b[0m, in \u001b[0;36mImageDataGenerator.flow\u001b[0;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    834\u001b[0m          x,\n\u001b[1;32m    835\u001b[0m          y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    842\u001b[0m          save_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    843\u001b[0m          subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    844\u001b[0m   \u001b[38;5;124;03m\"\"\"Takes data & label arrays, generates batches of augmented data.\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \n\u001b[1;32m    846\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    882\u001b[0m \n\u001b[1;32m    883\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 884\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNumpyArrayIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m      \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m      \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.9/site-packages/keras/preprocessing/image.py:463\u001b[0m, in \u001b[0;36mNumpyArrayIterator.__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, dtype)\u001b[0m\n\u001b[1;32m    461\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[1;32m    462\u001b[0m   kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m--> 463\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNumpyArrayIterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_data_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.9/site-packages/keras_preprocessing/image/numpy_array_iterator.py:86\u001b[0m, in \u001b[0;36mNumpyArrayIterator.__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, dtype)\u001b[0m\n\u001b[1;32m     83\u001b[0m     x_misc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`x` (images tensor) and `y` (labels) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     87\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould have the same length. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     88\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound: x.shape = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, y.shape = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m     89\u001b[0m                      (np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mshape, np\u001b[38;5;241m.\u001b[39masarray(y)\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sample_weight):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`x` (images tensor) and `sample_weight` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     92\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould have the same length. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     93\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound: x.shape = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, sample_weight.shape = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m     94\u001b[0m                      (np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mshape, np\u001b[38;5;241m.\u001b[39masarray(sample_weight)\u001b[38;5;241m.\u001b[39mshape))\n",
      "\u001b[0;31mValueError\u001b[0m: `x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (1964, 224, 224, 3), y.shape = (1835, 12)"
     ]
    }
   ],
   "source": [
    "data_gen = ImageDataGenerator(zoom_range=0.15, horizontal_flip=True, rotation_range=15, \n",
    "                              height_shift_range=0.3, width_shift_range=0.2)\n",
    "train_generator = data_gen.flow(X_train, y_test_stolen, batch_size=batch_size, shuffle=True)\n",
    "test_generator = data_gen.flow(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "62/62 [==============================] - 146s 2s/step - loss: 1.7386 - accuracy: 0.4842 - val_loss: 1.6957 - val_accuracy: 0.5600\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 166s 3s/step - loss: 0.9515 - accuracy: 0.7138 - val_loss: 1.4595 - val_accuracy: 0.5333\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 145s 2s/step - loss: 0.8312 - accuracy: 0.7429 - val_loss: 1.2076 - val_accuracy: 0.6333\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 139s 2s/step - loss: 0.7896 - accuracy: 0.7510 - val_loss: 1.0748 - val_accuracy: 0.6900\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 149s 2s/step - loss: 0.7653 - accuracy: 0.7469 - val_loss: 1.0642 - val_accuracy: 0.6933\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 175s 3s/step - loss: 0.6952 - accuracy: 0.7704 - val_loss: 1.1230 - val_accuracy: 0.6833\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 161s 3s/step - loss: 0.7011 - accuracy: 0.7576 - val_loss: 0.9125 - val_accuracy: 0.7600\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 166s 3s/step - loss: 0.6539 - accuracy: 0.7785 - val_loss: 0.9749 - val_accuracy: 0.7133\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 150s 2s/step - loss: 0.6279 - accuracy: 0.7795 - val_loss: 1.0003 - val_accuracy: 0.7200\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 158s 3s/step - loss: 0.5922 - accuracy: 0.7963 - val_loss: 0.9504 - val_accuracy: 0.7800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a740c3550>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_generator, epochs=epochs, validation_data=test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "62/62 [==============================] - 167s 3s/step - loss: 0.5782 - accuracy: 0.7892 - val_loss: 1.0214 - val_accuracy: 0.7367\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 164s 3s/step - loss: 0.5555 - accuracy: 0.8070 - val_loss: 0.9743 - val_accuracy: 0.7600\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 184s 3s/step - loss: 0.5633 - accuracy: 0.8004 - val_loss: 0.9469 - val_accuracy: 0.7767\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 198s 3s/step - loss: 0.5400 - accuracy: 0.8218 - val_loss: 0.9311 - val_accuracy: 0.7833\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 182s 3s/step - loss: 0.5386 - accuracy: 0.8060 - val_loss: 1.0384 - val_accuracy: 0.7667\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 159s 3s/step - loss: 0.5593 - accuracy: 0.8101 - val_loss: 0.9735 - val_accuracy: 0.7633\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 157s 3s/step - loss: 0.5047 - accuracy: 0.8243 - val_loss: 1.1136 - val_accuracy: 0.7433\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 167s 3s/step - loss: 0.4861 - accuracy: 0.8335 - val_loss: 0.9701 - val_accuracy: 0.7967\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 174s 3s/step - loss: 0.5102 - accuracy: 0.8167 - val_loss: 0.9374 - val_accuracy: 0.7933\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 148s 2s/step - loss: 0.4793 - accuracy: 0.8396 - val_loss: 0.9266 - val_accuracy: 0.7867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49f849e760>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_generator, epochs=epochs, validation_data=test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1964, 224, 224, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1964, 12])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_stolen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(train_generator, epochs=epochs, validation_data=test_generator, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22eaf92ddce55769a6f85d385c5961a233ee6fbddccfccc55ce01305794cc666"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
